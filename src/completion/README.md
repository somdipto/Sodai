# LLM Large Model Call Wrapper\n\n## chat/completions SSE Processing Logic\n\n> Standard parameter configuration (using deepseek as an example)\n\n### Request Initiation http.post\n\n- url: https://api.deepseek.com/chat/completions\n- key: API_KEY\n- model: deepseek-chat\n\n```js\nconst { url, API_KEY, model, temperature = 0 } = config;\nconst config = {\n  method: \"post\",\n  maxBodyLength: Infinity,\n  url,\n  headers: {\n    Authorization: `Bearer ${API_KEY}`,\n    \"Content-Type\": \"application/json\",\n  },\n  data: {\n    model, // model to call\n    messages, // chat prompt\n    stream: true, // streaming output\n    temperature,\n  },\n  responseType: \"stream\",\n};\n\nconst response = await axios.request(config).catch((err) => {\n  return err;\n});\nreturn response;\n```\n\n### Return Message Processing (SSE Streaming Processing)\n\n- splitter: \\n\\n\n- messageToValue: Read JSON.parse(message.split(\"data:\")[1]).choices[0].delta.content\n\n## Proxy Calls